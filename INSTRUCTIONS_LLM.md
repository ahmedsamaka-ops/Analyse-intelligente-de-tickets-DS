# Configuration du Chatbot LLM

## Options disponibles

### Option 1: Ollama (Recommand√© - Gratuit et Local) üè†

**Installation:**
```bash
# T√©l√©chargez Ollama depuis: https://ollama.ai/download
# Windows: t√©l√©chargez l'installeur

# Apr√®s installation, lancez Ollama
ollama serve

# Dans un autre terminal, t√©l√©chargez un mod√®le
ollama pull llama3.2
```

**Configuration:**
```bash
# Cr√©ez le fichier .env
cp .env.example .env

# Modifiez .env:
LLM_PROVIDER=ollama
OLLAMA_MODEL=llama3.2
```

### Option 2: OpenAI (Payant) üí∞

**Obtenir la cl√© API:**
1. Allez sur https://platform.openai.com/api-keys
2. Cr√©ez une cl√© API
3. Copiez la cl√©

**Configuration:**
```bash
# Dans .env:
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-votre-cle-ici
```

### Option 3: Mistral AI (Payant) üí∞

**Obtenir la cl√© API:**
1. Allez sur https://console.mistral.ai/
2. Cr√©ez une cl√© API
3. Copiez la cl√©

**Configuration:**
```bash
# Dans .env:
LLM_PROVIDER=mistral
MISTRAL_API_KEY=votre-cle-ici
```

## Installation des d√©pendances

```bash
# Activez votre environnement virtuel
.venv\Scripts\Activate.ps1

# Installez les packages n√©cessaires
pip install python-dotenv requests

# Si vous utilisez OpenAI:
pip install openai

# Si vous utilisez Mistral:
pip install mistralai
```

## Test du chatbot

```bash
python src/rag/chatbot.py
```

Le test posera la question "Quelle est la capitale du Maroc ?" et enregistrera le r√©sultat dans `llm_status.txt`.

## Utilisation dans votre code

```python
from src.rag.chatbot import chat

# Envoyez un message
response = chat("Comment r√©soudre un probl√®me de connexion ?")
print(response)
```
