"""
Script d'ingestion des donn√©es dans la base vectorielle
Utilise SentenceTransformers directement pour les embeddings
"""
import os
import sys
import pandas as pd
from pathlib import Path

# Ajouter le dossier parent au path pour les imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent))


def load_and_chunk_tickets(csv_path: str = "data/tickets.csv", max_chunks: int = 100):
    """
    Charge les tickets depuis le CSV et les pr√©pare en chunks
    
    Args:
        csv_path: Chemin vers le fichier CSV des tickets
        max_chunks: Nombre maximum de chunks √† cr√©er (pour test rapide)
    
    Returns:
        Liste de textes (chunks) et leurs m√©tadonn√©es
    """
    print(f"üìÇ Chargement des tickets depuis {csv_path}...")
    
    # Charger le CSV
    df = pd.read_csv(csv_path)
    print(f"   ‚úÖ {len(df)} tickets charg√©s")
    
    # Pr√©parer les chunks (texte complet du ticket)
    chunks = []
    metadatas = []
    
    for idx, row in df.head(max_chunks).iterrows():
        # Cr√©er un texte complet pour chaque ticket
        text = f"""Ticket ID: {row['ID']}
Titre: {row['titre']}
Cat√©gorie: {row['categorie']}
Urgence: {row['urgence']}
Type: {row['type_ticket']}
Temps de r√©solution: {row['temps_resolution']} heures

Texte: {row['text_full']}"""
        
        chunks.append(text)
        metadatas.append({
            "id": str(row['ID']),
            "categorie": row['categorie'],
            "urgence": row['urgence'],
            "type_ticket": row['type_ticket']
        })
    
    print(f"   ‚úÖ {len(chunks)} chunks cr√©√©s")
    return chunks, metadatas


def test_embeddings_with_langchain():
    """
    Test du mod√®le d'embeddings avec LangChain
    Note: N√©cessite l'installation de langchain-community
    """
    print("üîß Test avec LangChain (langchain_community.embeddings)...")
    
    try:
        from langchain_community.embeddings import SentenceTransformerEmbeddings
        
        # Charger le mod√®le
        embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
        
        print("‚úÖ LangChain charg√© avec succ√®s!")
        
        # Test avec un mot simple
        print("\nüìù Test d'embedding sur le mot 'bonjour'...")
        result = embeddings.embed_query("bonjour")
        
        print(f"\nüìä R√©sultat (vecteur de {len(result)} dimensions):")
        print(result[:10])  # Afficher les 10 premi√®res valeurs
        print("...")
        print(result[-10:])  # Afficher les 10 derni√®res valeurs
        
        print(f"\n‚úÖ Le mod√®le fonctionne correctement avec LangChain!")
        print(f"   Dimension du vecteur: {len(result)}")
        print(f"   Type: {type(result)}")
        
        return embeddings
    
    except ImportError as e:
        print(f"‚ùå Erreur d'import LangChain: {e}")
        print("   LangChain n√©cessite des d√©pendances avec Rust compiler")
        print("   Essai avec sentence-transformers directement...\n")
        return None


def test_embeddings_direct():
    """
    Test du mod√®le d'embeddings directement avec sentence-transformers
    Alternative qui fonctionne sans LangChain
    """
    print("üîß Chargement du mod√®le d'embeddings (sentence-transformers)...")
    
    try:
        from sentence_transformers import SentenceTransformer
        
        # Charger le mod√®le
        model = SentenceTransformer("all-MiniLM-L6-v2")
        
        print("‚úÖ Mod√®le charg√© avec succ√®s!")
        
        # Test avec un mot simple
        print("\nüìù Test d'embedding sur le mot 'bonjour'...")
        result = model.encode("bonjour")
        
        print(f"\nüìä R√©sultat (vecteur de {len(result)} dimensions):")
        print(result[:10])  # Afficher les 10 premi√®res valeurs
        print("...")
        print(result[-10:])  # Afficher les 10 derni√®res valeurs
        
        print(f"\n‚úÖ Le mod√®le fonctionne correctement!")
        print(f"   Dimension du vecteur: {len(result)}")
        print(f"   Type: {type(result)}")
        
        # Test avec une phrase
        print("\n" + "="*60)
        print("üìù Test avec une phrase compl√®te...")
        phrase = "Comment r√©soudre un probl√®me de connexion Maroc Telecom?"
        result2 = model.encode(phrase)
        
        print(f"Phrase: {phrase}")
        print(f"Vecteur de {len(result2)} dimensions g√©n√©r√©")
        print(f"Premiers √©l√©ments: {result2[:5]}")
        
        return model
    
    except ImportError as e:
        print(f"‚ùå Erreur: {e}")
        print("   Installez sentence-transformers: pip install sentence-transformers")
        return None
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement: {e}")
        return None


def create_chroma_db_with_langchain(chunks, metadatas):
    """
    Cr√©e une base Chroma avec LangChain
    """
    print("\n" + "="*60)
    print("üóÑÔ∏è CR√âATION DE LA BASE CHROMA (LangChain)")
    print("="*60)
    
    try:
        from langchain_community.vectorstores import Chroma
        from langchain_community.embeddings import SentenceTransformerEmbeddings
        
        # Charger le mod√®le d'embeddings
        print("\nüì¶ Chargement du mod√®le d'embeddings...")
        embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
        print("   ‚úÖ Mod√®le charg√©")
        
        # Cr√©er le dossier si n√©cessaire
        persist_directory = "./chroma_db"
        os.makedirs(persist_directory, exist_ok=True)
        
        # Cr√©er la base Chroma
        print(f"\nüî® Cr√©ation de la base Chroma avec {len(chunks)} documents...")
        db = Chroma.from_texts(
            texts=chunks,
            embedding=embeddings,
            metadatas=metadatas,
            persist_directory=persist_directory
        )
        
        print(f"   ‚úÖ Base cr√©√©e dans {persist_directory}")
        
        # Test de recherche
        print("\n" + "="*60)
        print("üîç TEST DE RECHERCHE DE SIMILARIT√â")
        print("="*60)
        
        query = "probleme wifi"
        print(f"\n‚ùì Requ√™te: '{query}'")
        print(f"üìä Recherche des 3 documents les plus similaires...")
        
        results = db.similarity_search(query, k=3)
        
        print(f"\n‚úÖ {len(results)} r√©sultats trouv√©s:\n")
        
        for i, doc in enumerate(results, 1):
            print(f"{'='*60}")
            print(f"R√©sultat {i}:")
            print(f"{'='*60}")
            print(doc.page_content[:200] + "...")
            if doc.metadata:
                print(f"\nM√©tadonn√©es: {doc.metadata}")
            print()
        
        print("="*60)
        print("‚úÖ La base Chroma fonctionne correctement!")
        print(f"üìÇ Dossier cr√©√©: {persist_directory}")
        print("="*60)
        
        return db
    
    except ImportError as e:
        print(f"\n‚ùå Erreur d'import: {e}")
        print("   Packages requis: langchain-community, chromadb, sentence-transformers")
        print("   Ces packages n√©cessitent un compilateur Rust sur Python 3.14")
        return None
    except Exception as e:
        print(f"\n‚ùå Erreur lors de la cr√©ation: {e}")
        return None


def create_simple_vector_db(chunks, metadatas):
    """
    Cr√©e une base vectorielle simple avec scikit-learn (TF-IDF)
    Alternative qui fonctionne sans d√©pendances complexes
    """
    print("\n" + "="*60)
    print("üóÑÔ∏è CR√âATION DE LA BASE VECTORIELLE (TF-IDF)")
    print("="*60)
    
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        import pickle
        
        # Cr√©er le vectorizer TF-IDF
        print("\nüì¶ Cr√©ation du vectorizer TF-IDF...")
        vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
        
        # Vectoriser les chunks
        print(f"üî® Vectorisation de {len(chunks)} documents...")
        tfidf_matrix = vectorizer.fit_transform(chunks)
        print(f"   ‚úÖ Matrice TF-IDF cr√©√©e: {tfidf_matrix.shape}")
        
        # Sauvegarder
        db_path = "./simple_vector_db"
        os.makedirs(db_path, exist_ok=True)
        
        with open(f"{db_path}/vectorizer.pkl", "wb") as f:
            pickle.dump(vectorizer, f)
        with open(f"{db_path}/matrix.pkl", "wb") as f:
            pickle.dump(tfidf_matrix, f)
        with open(f"{db_path}/chunks.pkl", "wb") as f:
            pickle.dump(chunks, f)
        with open(f"{db_path}/metadatas.pkl", "wb") as f:
            pickle.dump(metadatas, f)
        
        print(f"   ‚úÖ Base sauvegard√©e dans {db_path}")
        
        # Test de recherche
        print("\n" + "="*60)
        print("üîç TEST DE RECHERCHE DE SIMILARIT√â")
        print("="*60)
        
        query = "probleme wifi"
        print(f"\n‚ùì Requ√™te: '{query}'")
        
        # Vectoriser la query
        query_vec = vectorizer.transform([query])
        
        # Calculer les similarit√©s
        similarities = cosine_similarity(query_vec, tfidf_matrix)[0]
        
        # Trouver les top 3
        top_indices = similarities.argsort()[-3:][::-1]
        
        print(f"üìä Top 3 r√©sultats les plus similaires:\n")
        
        for i, idx in enumerate(top_indices, 1):
            print(f"{'='*60}")
            print(f"R√©sultat {i} (similarit√©: {similarities[idx]:.3f}):")
            print(f"{'='*60}")
            print(chunks[idx][:200] + "...")
            if metadatas[idx]:
                print(f"\nM√©tadonn√©es: {metadatas[idx]}")
            print()
        
        print("="*60)
        print("‚úÖ La base vectorielle TF-IDF fonctionne correctement!")
        print(f"üìÇ Dossier cr√©√©: {db_path}")
        print("="*60)
        
        return {
            'vectorizer': vectorizer,
            'matrix': tfidf_matrix,
            'chunks': chunks,
            'metadatas': metadatas
        }
    
    except Exception as e:
        print(f"\n‚ùå Erreur: {e}")
        return None


if __name__ == "__main__":
    print("="*60)
    print(" INGESTION DES DONN√âES DANS LA BASE VECTORIELLE")
    print("="*60)
    print()
    
    # √âtape 1: Charger et chunker les tickets
    chunks, metadatas = load_and_chunk_tickets(max_chunks=50)
    
    # √âtape 2: Essayer d'abord avec LangChain
    result_langchain = test_embeddings_with_langchain()
    
    if result_langchain is not None:
        # Si LangChain fonctionne, cr√©er la base Chroma
        db = create_chroma_db_with_langchain(chunks, metadatas)
    else:
        # Sinon, utiliser la solution simple TF-IDF
        print("\n‚ö†Ô∏è LangChain/Chroma non disponible")
        print("   Utilisation de la solution alternative (TF-IDF)...\n")
        db = create_simple_vector_db(chunks, metadatas)
    
    if db is not None:
        print("\n" + "="*60)
        print("‚úÖ SUCC√àS - Base vectorielle cr√©√©e et test√©e!")
        print("="*60)
    else:
        print("\n" + "="*60)
        print("‚ùå √âCHEC - Impossible de cr√©er la base vectorielle")
        print("="*60)
