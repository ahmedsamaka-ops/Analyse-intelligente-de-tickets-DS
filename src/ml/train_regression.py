# =============================================================================
# Script d'Entra√Ænement du Mod√®le de R√©gression
# Projet : Analyse Intelligente de Tickets Support
# Auteur : Expert ML
# Date : F√©vrier 2026
# =============================================================================
# Ce script entra√Æne un mod√®le de r√©gression pour pr√©dire le temps de r√©solution
# d'un ticket support en heures.
# =============================================================================

import pandas as pd
import numpy as np
import re
import joblib
import os
from datetime import datetime

# Scikit-learn imports
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    mean_absolute_percentage_error
)

import warnings
warnings.filterwarnings('ignore')

# =============================================================================
# CONFIGURATION
# =============================================================================
DATA_DIR = "data"
MODELS_DIR = "models"
TRAIN_FILE = os.path.join(DATA_DIR, "train.csv")
TEST_FILE = os.path.join(DATA_DIR, "test.csv")
VALIDATION_FILE = os.path.join(DATA_DIR, "validation.csv")

# Cr√©er le dossier models s'il n'existe pas
os.makedirs(MODELS_DIR, exist_ok=True)

# Seed pour la reproductibilit√©
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# =============================================================================
# 1. FONCTIONS DE PR√âTRAITEMENT
# =============================================================================

def nettoyer_texte(texte):
    """
    Nettoie un texte pour l'analyse ML.
    """
    if pd.isna(texte):
        return ""
    
    texte = str(texte)
    
    # Corrections d'encodage
    corrections = {
        '√É¬©': '√©', '√É¬®': '√®', '√É¬™': '√™', '√É ': '√†',
        '√É¬ß': '√ß', '√É¬¥': '√¥', '√É¬Æ': '√Æ', '√É¬Ø': '√Ø',
        '√É¬π': '√π', '√É¬ª': '√ª', '√É¬¢': '√¢', '√¢‚Ç¨‚Ñ¢': "'",
    }
    for ancien, nouveau in corrections.items():
        texte = texte.replace(ancien, nouveau)
    
    texte = texte.lower()
    texte = re.sub(r'[^\w\s\-√†√¢√§√©√®√™√´√Ø√Æ√¥√π√ª√º√ß]', ' ', texte)
    texte = re.sub(r'\s+', ' ', texte).strip()
    
    return texte


def charger_donnees():
    """
    Charge les donn√©es d'entra√Ænement, test et validation.
    """
    print("=" * 70)
    print("üìÇ CHARGEMENT DES DONN√âES")
    print("=" * 70)
    
    train_df = pd.read_csv(TRAIN_FILE)
    test_df = pd.read_csv(TEST_FILE)
    val_df = pd.read_csv(VALIDATION_FILE)
    
    print(f"‚úÖ Train     : {len(train_df):,} tickets")
    print(f"‚úÖ Test      : {len(test_df):,} tickets")
    print(f"‚úÖ Validation: {len(val_df):,} tickets")
    
    return train_df, test_df, val_df


def analyser_temps_resolution(df):
    """
    Analyse statistique du temps de r√©solution.
    """
    print("\nüìä Statistiques du temps de r√©solution (heures) :")
    temps = df['temps_resolution']
    
    print(f"   - Minimum  : {temps.min():.2f}h")
    print(f"   - Maximum  : {temps.max():.2f}h")
    print(f"   - Moyenne  : {temps.mean():.2f}h")
    print(f"   - M√©diane  : {temps.median():.2f}h")
    print(f"   - √âcart-type: {temps.std():.2f}h")
    
    # Percentiles
    print(f"\n   Percentiles :")
    for p in [25, 50, 75, 90, 95]:
        print(f"      {p}% : {temps.quantile(p/100):.2f}h")
    
    return temps


def preparer_features(df, tfidf_vectorizer=None, label_encoders=None, fit=True):
    """
    Pr√©pare les features pour la r√©gression.
    
    Combine :
    - Features textuelles (TF-IDF)
    - Features cat√©gorielles encod√©es (cat√©gorie, urgence, type_ticket)
    - Features num√©riques (nb_mots)
    
    Returns:
        X: Matrice de features
        vectorizer: TF-IDF vectorizer
        encoders: Dictionnaire des label encoders
    """
    df = df.copy()
    
    # Nettoyer le texte
    df['texte_clean'] = df['texte'].apply(nettoyer_texte)
    
    # 1. Features TF-IDF
    if fit:
        tfidf_vectorizer = TfidfVectorizer(
            max_features=2000,
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.9
        )
        X_tfidf = tfidf_vectorizer.fit_transform(df['texte_clean'])
    else:
        X_tfidf = tfidf_vectorizer.transform(df['texte_clean'])
    
    # 2. Features cat√©gorielles
    if fit:
        label_encoders = {}
        
    categorical_features = []
    for col in ['categorie', 'urgence', 'type_ticket']:
        if col in df.columns:
            if fit:
                le = LabelEncoder()
                encoded = le.fit_transform(df[col].fillna('Inconnu'))
                label_encoders[col] = le
            else:
                # G√©rer les valeurs inconnues
                le = label_encoders[col]
                encoded = df[col].fillna('Inconnu').apply(
                    lambda x: le.transform([x])[0] if x in le.classes_ else -1
                ).values
            categorical_features.append(encoded.reshape(-1, 1))
    
    # 3. Features num√©riques
    numeric_features = []
    if 'nb_mots' in df.columns:
        numeric_features.append(df['nb_mots'].fillna(0).values.reshape(-1, 1))
    
    # Combiner toutes les features
    X_tfidf_dense = X_tfidf.toarray()
    
    all_features = [X_tfidf_dense]
    all_features.extend(categorical_features)
    all_features.extend(numeric_features)
    
    X = np.hstack(all_features)
    
    return X, tfidf_vectorizer, label_encoders


# =============================================================================
# 2. ENTRA√éNEMENT DU MOD√àLE DE R√âGRESSION
# =============================================================================

def entrainer_regresseur(train_df, test_df, val_df):
    """
    Entra√Æne un mod√®le de r√©gression pour pr√©dire le temps de r√©solution.
    
    Returns:
        Dictionnaire avec le mod√®le, le vectoriseur, les encodeurs et les m√©triques
    """
    print("\n" + "=" * 70)
    print("‚è±Ô∏è  R√âGRESSION - PR√âDICTION DU TEMPS DE R√âSOLUTION")
    print("=" * 70)
    
    # Analyse des donn√©es
    analyser_temps_resolution(train_df)
    
    # Pr√©parer les features
    print("\nüîÑ Pr√©paration des features...")
    X_train, tfidf_vectorizer, label_encoders = preparer_features(
        train_df, fit=True
    )
    y_train = train_df['temps_resolution'].values
    
    X_test, _, _ = preparer_features(
        test_df, tfidf_vectorizer, label_encoders, fit=False
    )
    y_test = test_df['temps_resolution'].values
    
    X_val, _, _ = preparer_features(
        val_df, tfidf_vectorizer, label_encoders, fit=False
    )
    y_val = val_df['temps_resolution'].values
    
    print(f"‚úÖ Shape X_train : {X_train.shape}")
    print(f"‚úÖ Shape X_test  : {X_test.shape}")
    print(f"‚úÖ Shape X_val   : {X_val.shape}")
    
    # Comparaison de plusieurs mod√®les
    print("\nüî¨ Comparaison des mod√®les de r√©gression...")
    
    modeles = {
        'Random Forest': RandomForestRegressor(
            n_estimators=200,
            max_depth=15,
            min_samples_split=5,
            random_state=RANDOM_STATE,
            n_jobs=-1
        ),
        'Gradient Boosting': GradientBoostingRegressor(
            n_estimators=100,
            max_depth=8,
            learning_rate=0.1,
            random_state=RANDOM_STATE
        ),
        'Ridge': Ridge(alpha=1.0, random_state=RANDOM_STATE)
    }
    
    meilleurs_resultats = {'modele': None, 'nom': '', 'rmse': float('inf')}
    
    for nom, modele in modeles.items():
        print(f"\n   üìä {nom}...")
        modele.fit(X_train, y_train)
        y_pred = modele.predict(X_test)
        
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        print(f"      RMSE : {rmse:.2f}h")
        print(f"      MAE  : {mae:.2f}h")
        print(f"      R¬≤   : {r2:.4f}")
        
        if rmse < meilleurs_resultats['rmse']:
            meilleurs_resultats = {
                'modele': modele,
                'nom': nom,
                'rmse': rmse
            }
    
    # Utiliser le meilleur mod√®le
    meilleur_modele = meilleurs_resultats['modele']
    print(f"\nüèÜ Meilleur mod√®le : {meilleurs_resultats['nom']}")
    
    # √âvaluation finale sur le set de validation
    print("\n" + "-" * 50)
    print("üìà √âVALUATION FINALE (Set de Validation)")
    print("-" * 50)
    
    y_val_pred = meilleur_modele.predict(X_val)
    
    # Appliquer les contraintes m√©tier
    # Temps minimum : 0.5h, Temps maximum : 168h (1 semaine)
    y_val_pred = np.clip(y_val_pred, 0.5, 168)
    
    # M√©triques
    rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
    mae = mean_absolute_error(y_val, y_val_pred)
    r2 = r2_score(y_val, y_val_pred)
    
    # MAPE (Mean Absolute Percentage Error) - √©viter division par z√©ro
    mask = y_val > 0
    if mask.sum() > 0:
        mape = np.mean(np.abs((y_val[mask] - y_val_pred[mask]) / y_val[mask])) * 100
    else:
        mape = 0
    
    print(f"\n   üìä RMSE  : {rmse:.2f} heures")
    print(f"   üìä MAE   : {mae:.2f} heures")
    print(f"   üìä R¬≤    : {r2:.4f} ({r2*100:.2f}%)")
    print(f"   üìä MAPE  : {mape:.2f}%")
    
    # Analyse des erreurs
    erreurs = np.abs(y_val - y_val_pred)
    print(f"\n   üìä Analyse des erreurs absolues :")
    print(f"      - Erreur moyenne     : {erreurs.mean():.2f}h")
    print(f"      - Erreur m√©diane     : {np.median(erreurs):.2f}h")
    print(f"      - 90% des erreurs <  : {np.percentile(erreurs, 90):.2f}h")
    
    # Pourcentage de pr√©dictions dans une marge acceptable
    for marge in [2, 5, 10, 24]:
        pct = (erreurs <= marge).mean() * 100
        print(f"      - Erreur <= {marge:2d}h     : {pct:.1f}%")
    
    # Cross-validation
    print("\n   üîÑ Cross-Validation (5-fold) sur Train...")
    cv_scores = cross_val_score(
        meilleur_modele, X_train, y_train, 
        cv=5, scoring='neg_root_mean_squared_error'
    )
    cv_rmse = -cv_scores
    print(f"      RMSE CV : {cv_rmse}")
    print(f"      Moyenne : {cv_rmse.mean():.2f}h (+/- {cv_rmse.std()*2:.2f}h)")
    
    # Feature importance (si Random Forest)
    if hasattr(meilleur_modele, 'feature_importances_'):
        print("\n   üîç Top 10 Features les plus importantes :")
        importances = meilleur_modele.feature_importances_
        
        # Cr√©er les noms des features
        feature_names = list(tfidf_vectorizer.get_feature_names_out())
        feature_names.extend(['categorie_encoded', 'urgence_encoded', 'type_ticket_encoded', 'nb_mots'])
        
        # Trier par importance
        indices = np.argsort(importances)[::-1][:10]
        for i, idx in enumerate(indices):
            if idx < len(feature_names):
                print(f"      {i+1:2d}. {feature_names[idx][:30]:<30} : {importances[idx]:.4f}")
    
    # Sauvegarder les mod√®les
    print("\nüíæ Sauvegarde des mod√®les...")
    
    modele_path = os.path.join(MODELS_DIR, "regression_temps_model.pkl")
    vectorizer_path = os.path.join(MODELS_DIR, "tfidf_vectorizer_regression.pkl")
    encoders_path = os.path.join(MODELS_DIR, "label_encoders_regression.pkl")
    
    joblib.dump(meilleur_modele, modele_path)
    joblib.dump(tfidf_vectorizer, vectorizer_path)
    joblib.dump(label_encoders, encoders_path)
    
    print(f"   ‚úÖ Mod√®le     : {modele_path}")
    print(f"   ‚úÖ Vectorizer : {vectorizer_path}")
    print(f"   ‚úÖ Encoders   : {encoders_path}")
    
    return {
        'modele': meilleur_modele,
        'vectorizer': tfidf_vectorizer,
        'encoders': label_encoders,
        'metriques': {
            'rmse': rmse,
            'mae': mae,
            'r2': r2,
            'mape': mape,
            'cv_rmse_mean': cv_rmse.mean(),
            'cv_rmse_std': cv_rmse.std()
        },
        'nom_modele': meilleurs_resultats['nom']
    }


# =============================================================================
# 3. FONCTION DE PR√âDICTION
# =============================================================================

def predire_temps_resolution(texte, categorie='Autre', urgence='Basse', type_ticket='Demande', nb_mots=None):
    """
    Pr√©dit le temps de r√©solution d'un ticket.
    
    Args:
        texte: Le texte du ticket
        categorie: La cat√©gorie du ticket
        urgence: Le niveau d'urgence
        type_ticket: Type (Demande ou Incident)
        nb_mots: Nombre de mots (calcul√© si None)
        
    Returns:
        Dictionnaire avec la pr√©diction et l'intervalle de confiance
    """
    # Charger les mod√®les
    modele = joblib.load(os.path.join(MODELS_DIR, "regression_temps_model.pkl"))
    vectorizer = joblib.load(os.path.join(MODELS_DIR, "tfidf_vectorizer_regression.pkl"))
    encoders = joblib.load(os.path.join(MODELS_DIR, "label_encoders_regression.pkl"))
    
    # Pr√©parer le texte
    texte_clean = nettoyer_texte(texte)
    
    # Calculer nb_mots si non fourni
    if nb_mots is None:
        nb_mots = len(texte_clean.split())
    
    # Cr√©er un DataFrame temporaire
    temp_df = pd.DataFrame({
        'texte': [texte],
        'categorie': [categorie],
        'urgence': [urgence],
        'type_ticket': [type_ticket],
        'nb_mots': [nb_mots]
    })
    
    # Pr√©parer les features
    X, _, _ = preparer_features(temp_df, vectorizer, encoders, fit=False)
    
    # Pr√©diction
    prediction = modele.predict(X)[0]
    
    # Appliquer les contraintes m√©tier
    prediction = np.clip(prediction, 0.5, 168)
    
    # Calculer l'intervalle de confiance (¬±20% par d√©faut)
    marge = 0.20
    intervalle_min = max(0.5, prediction * (1 - marge))
    intervalle_max = min(168, prediction * (1 + marge))
    
    return {
        'temps_estime_heures': float(prediction),
        'temps_estime_jours': float(prediction / 24),
        'intervalle_confiance': {
            'min_heures': float(intervalle_min),
            'max_heures': float(intervalle_max),
            'min_jours': float(intervalle_min / 24),
            'max_jours': float(intervalle_max / 24)
        }
    }


# =============================================================================
# 4. MAIN - EX√âCUTION PRINCIPALE
# =============================================================================

if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("üöÄ ENTRA√éNEMENT DU MOD√àLE DE R√âGRESSION")
    print("   Projet : Analyse Intelligente de Tickets Support")
    print("   Date   :", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print("=" * 70)
    
    # Charger les donn√©es
    train_df, test_df, val_df = charger_donnees()
    
    # Entra√Æner le r√©gresseur
    resultats = entrainer_regresseur(train_df, test_df, val_df)
    
    # R√©sum√© final
    print("\n" + "=" * 70)
    print("üìä R√âSUM√â FINAL - MOD√àLE DE R√âGRESSION")
    print("=" * 70)
    
    print(f"\n‚è±Ô∏è  R√©gression TEMPS DE R√âSOLUTION :")
    print(f"   - Mod√®le utilis√© : {resultats['nom_modele']}")
    print(f"   - RMSE           : {resultats['metriques']['rmse']:.2f} heures")
    print(f"   - MAE            : {resultats['metriques']['mae']:.2f} heures")
    print(f"   - R¬≤             : {resultats['metriques']['r2']*100:.2f}%")
    print(f"   - MAPE           : {resultats['metriques']['mape']:.2f}%")
    print(f"   - CV RMSE        : {resultats['metriques']['cv_rmse_mean']:.2f}h (+/- {resultats['metriques']['cv_rmse_std']*2:.2f}h)")
    
    print("\nüìÅ Fichiers sauvegard√©s dans le dossier 'models/':")
    for f in os.listdir(MODELS_DIR):
        if 'regression' in f.lower() and f.endswith('.pkl'):
            size = os.path.getsize(os.path.join(MODELS_DIR, f)) / 1024
            print(f"   - {f} ({size:.1f} KB)")
    
    # Test avec des exemples
    print("\n" + "=" * 70)
    print("üß™ TESTS AVEC DES EXEMPLES")
    print("=" * 70)
    
    exemples = [
        {
            'texte': "probl√®me de connexion wifi impossible de se connecter",
            'categorie': 'Connexion internet',
            'urgence': 'Moyenne',
            'type_ticket': 'Incident'
        },
        {
            'texte': "demande de cr√©ation d'un compte active directory pour nouveau collaborateur",
            'categorie': 'Cr√©ation compte AD',
            'urgence': 'Basse',
            'type_ticket': 'Demande'
        },
        {
            'texte': "panne totale r√©seau urgent toute l'√©quipe bloqu√©e",
            'categorie': 'Connexion internet',
            'urgence': 'Tr√®s haute',
            'type_ticket': 'Incident'
        }
    ]
    
    for i, ex in enumerate(exemples, 1):
        print(f"\nüìù Exemple {i} : \"{ex['texte'][:50]}...\"")
        print(f"   Cat√©gorie : {ex['categorie']}, Urgence : {ex['urgence']}")
        
        pred = predire_temps_resolution(
            ex['texte'], 
            ex['categorie'], 
            ex['urgence'], 
            ex['type_ticket']
        )
        
        print(f"   ‚è±Ô∏è  Temps estim√© : {pred['temps_estime_heures']:.1f}h ({pred['temps_estime_jours']:.1f} jours)")
        print(f"   üìä Intervalle   : [{pred['intervalle_confiance']['min_heures']:.1f}h - {pred['intervalle_confiance']['max_heures']:.1f}h]")
    
    print("\n" + "=" * 70)
    print("‚úÖ ENTRA√éNEMENT TERMIN√â AVEC SUCC√àS !")
    print("=" * 70)
